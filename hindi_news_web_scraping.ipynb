{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88141ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important library\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67dc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL to exract news\n",
    "url = 'https://www.aajtak.in/uttar-pradesh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad4ae8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing selenium libraries for automation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "\n",
    "\n",
    "# Function to extract data from a single page\n",
    "def extract_data_from_page(soup):\n",
    "    h2_tags = soup.find_all('h2')\n",
    "    data_list = []\n",
    "    for h2_tag in h2_tags:\n",
    "        anchor_tag = h2_tag.a\n",
    "        if anchor_tag:\n",
    "            title = anchor_tag.text\n",
    "            link = anchor_tag['href']\n",
    "            data_dict = {\n",
    "                \"title\": title,\n",
    "                \"link\": link\n",
    "            }\n",
    "            data_list.append(data_dict)\n",
    "    return data_list\n",
    "\n",
    "# Function to click on the \"Load More\" button and wait for new content to load\n",
    "def click_load_more_button(driver):\n",
    "    try:\n",
    "        load_more_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"load_more\"))\n",
    "        )\n",
    "        load_more_button.click()\n",
    "        # Wait for new content to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"h2 a\"))\n",
    "        )\n",
    "        return True\n",
    "    except StaleElementReferenceException:\n",
    "        return False\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "# Set up the WebDriver (you can use other browser drivers if desired)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Extract data from the initial page\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "data_list = extract_data_from_page(soup)\n",
    "\n",
    "# Loop to click \"Load More\" and extract additional data until no more content is loaded\n",
    "while click_load_more_button(driver):\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    new_data_list = extract_data_from_page(soup)\n",
    "    data_list.extend(new_data_list)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2ade447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from extracted text\n",
    "link_data = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "751c956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting clean hindi text \n",
    "import re\n",
    "\n",
    "def extract_hindi_text(input_text):\n",
    "    # Define the regex pattern to match Hindi characters (Devanagari script)\n",
    "    hindi_pattern = r'[\\u0900-\\u097F]+'\n",
    "    \n",
    "    # Use re.findall to find all occurrences of the pattern in the input text\n",
    "    hindi_text_list = re.findall(hindi_pattern, input_text)\n",
    "    \n",
    "    # Join the list of matches into a single string\n",
    "    hindi_text = ' '.join(hindi_text_list)\n",
    "    \n",
    "    return hindi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26f504da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for news in link_data['link']:\n",
    "    news_page = r.get(news).text\n",
    "    data = bs(news_page,'lxml')\n",
    "    para = data.find_all('p')\n",
    "    for i in para:\n",
    "        clean = extract_hindi_text(i.text)\n",
    "        text.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe35738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
